
@article{BCM:1982,
author = {Bienenstock, Elie and Cooper, Leon and Munro, Paul},
year = {1982},
month = {02},
pages = {32-48},
title = {Theory for the development of neuron selectivity: Orientation specificity and binocular interaction in visual cortex},
volume = {2},
isbn = {978-981-02-1814-0},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
doi = {10.1142/9789812795885_0006}
}

@article{law:1994,
author = {C C Law  and L N Cooper },
title = {Formation of receptive fields in realistic visual environments according to the Bienenstock, Cooper, and Munro (BCM) theory.},
journal = {Proceedings of the National Academy of Sciences},
volume = {91},
number = {16},
pages = {7797-7801},
year = {1994},
doi = {10.1073/pnas.91.16.7797},
%url={https://www.pnas.org/doi/abs/10.1073/pnas.91.16.7797},
%eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.91.16.7797},
abstract = {The Bienenstock, Cooper, and Munro (BCM) theory of synaptic plasticity has successfully reproduced the development of orientation selectivity and ocular dominance in kitten visual cortex in normal, as well as deprived, visual environments. To better compare the consequences of this theory with experiment, previous abstractions of the visual environment are replaced in this work by real visual images with retinal processing. The visual environment is represented by 24 gray-scale natural images that are shifted across retinal fields. In this environment, the BCM neuron develops receptive fields similar to the fields of simple cells found in kitten striate cortex. These fields display adjacent excitatory and inhibitory bands when tested with spot stimuli, orientation selectivity when tested with bar stimuli, and spatial-frequency selectivity when tested with sinusoidal gratings. In addition, their development in various deprived visual environments agrees with experimental results.}}

@article{castellani:1998,
author = {G C Castellani, N Intrator, H Shouval and L N Cooper},
title = {Solutions of the BCM learning rule in a network of lateral interacting nonlinear neurons},
journal = {Network: Computation in Neural Systems},
volume = {10},
number = {2},
pages = {111-121},
year = {1999},
publisher = {Taylor & Francis},
doi = {10.1088/0954-898X\_10\_2\_001},
%URL = {https://doi.org/10.1088/0954-898X_10_2_001},
%eprint = {https://doi.org/10.1088/0954-898X_10_2_001}
}

@article{cooper:1988,
author = {L N Cooper  and C L Scofield },
title = {Mean-field theory of a neural network.},
journal = {Proceedings of the National Academy of Sciences},
volume = {85},
number = {6},
pages = {1973-1977},
year = {1988},
doi = {10.1073/pnas.85.6.1973},
%URL = {https://www.pnas.org/doi/abs/10.1073/pnas.85.6.1973},
%eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.85.6.1973},
abstract = {A single-cell theory for the development of selectivity and ocular dominance in visual cortex has been generalized to incorporate more realistic neural networks that approximate the actual anatomy of small regions of cortex. In particular, we have analyzed a network consisting of excitatory and inhibitory cells, both of which may receive information from the lateral geniculate nucleus (LGN) and then interact through cortico-cortical synapses in a mean-field approximation. Our investigation of the evolution of a cell in this mean-field network indicates that many of the results on existence and stability of fixed points that have been obtained previously in the single-cell theory can be successfully generalized here. We can, in addition, make explicit further statements concerning the independent effects of excitatory and inhibitory neurons on selectivity and ocular dominance. For example, shutting off inhibitory cells lessens selectivity and alters ocular dominance (masked synapses). These inhibitory cells may be selective, but there is no theoretical necessity that they be so. Further, the intercortical inhibitory synapses do not have to be very responsive to visual experience. Most of the learning process can occur among the excitatory LGN-cortical synapses.}}


@misc{berselli:2023,
  author = {Berselli, Gregorio},
  title = {Memorization capability of the BCM model.},
  year = {2023},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/Grufoony/BCM_selectivity}}
}


@article{heusel:2017,
author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
title = {GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
URL = {https://dl.acm.org/doi/10.5555/3295222.3295408},
abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the 'Fr\'{e}chet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {6629â€“6640},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS'17}
}


@article{naeem:2020,
author = {Naeem, Muhammad Ferjad and Oh, Seong Joon and Uh, Youngjung and Choi, Yunjey and Yoo, Jaejun},
title = {Reliable Fidelity and Diversity Metrics for Generative Models},
year = {2020},
publisher = {JMLR.org},
doi = {10.48550/arXiv.2002.09797},
abstract = {Devising indicative evaluation metrics for the image generation task remains an open problem. The most widely used metric for measuring the similarity between real and generated images has been the Fr\'{e}chet Inception Distance (FID) score. Because it does not differentiate the fidelity and diversity aspects of the generated images, recent papers have introduced variants of precision and recall metrics to diagnose those properties separately. In this paper, we show that even the latest version of the precision and recall metrics are not reliable yet; for example, they fail to detect the match between two identical distributions, they are not robust against outliers, and the evaluation hyperparameters are selected arbitrarily. We propose density and coverage metrics that solve the above issues. We analytically and experimentally show that density and coverage provide more interpretable and reliable signals for practitioners than the existing metrics. Code: github.com/clovaai/generative-evaluation-prdc.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {665},
numpages = {10},
series = {ICML'20}
}


@misc{curti:2020,
  author = {Curti, Nico and Squadrani, Lorenzo and Gasperini, Simone and Ceccarelli, Mattia},
  title = {plasticity - Unsupervised Neural Networks with biological-inspired learning rules},
  year = {2020},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/Nico-Curti/plasticity}}
}


@misc{seitzer:2020,
  author={Seitzer, Maximilian},
  title={{pytorch-fid: FID Score for PyTorch}},
  month={8},
  year={2020},
  note={Version 0.3.0},
  howpublished={\url{https://github.com/mseitzer/pytorch-fid}},
}